{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d5298",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instalação e importação dos pacotes necessários para extração e requisição dos dados, classificação das etnias\n",
    "# e manipulação da tabela\n",
    "# Installation and importantion of necessary packages for extraction and retrieval, race classification and table manipulation\n",
    "\n",
    "!pip3 install opencv-python deepface numpy selenium\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import  time\n",
    "import cv2 \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3bbe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variável que evita o bloqueio do código pelo site\n",
    "# Variable that prevents the code from being blocket by the website\n",
    "\n",
    "headers = {'User-Agent': \n",
    "           'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3138cbb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Essas listas serão o resultado da extração. Você pode adicionar novas listas incluindo a quantidade de partidas etc.\n",
    "# These lists will be the result of the extraction. You can add new lists, such as the number of matches etc.\n",
    "Lista_de_Jogadores = []\n",
    "Gols_Marcados = []\n",
    "Fotos = []\n",
    "\n",
    "# Criação de uma função que fará a extração dos dados\n",
    "# Creation of a function that will perform dates extraction\n",
    "\n",
    "def extraçao():\n",
    "    page = input(\"Insira a página que terá os dados extraídos:\\n\")\n",
    "    pageTree = requests.get(page, headers=headers)\n",
    "    \n",
    "# Na variável \"driver\", é necessário indicar o caminho no diretório do seu computador onde está o driver utilizado\n",
    "# In the \"driver\" variable, it will be necessary to indicate the patch in yout computer's directory where the driver is located\n",
    "\n",
    "    driver = webdriver.Chrome(r'C:\\Users\\arthu\\Downloads\\chromedriver_win32 (1)\\chromedriver.exe')\n",
    "    \n",
    "# A passagem no selenium é feita a partir da localização do elemento com o título \"Para a página seguinte\"\n",
    "# The \"click\" in selenium was made from localization of element \"Para a página seguinte\"\n",
    "    driver.implicitly_wait(30)\n",
    "    driver.get(page)\n",
    "    \n",
    "# O código de extração\n",
    "# The extraction code\n",
    "    while True:\n",
    "        try:\n",
    "            next_page_element = WebDriverWait(driver, 10).until(\n",
    "                \n",
    "# A passagem no selenium é feita a partir da localização do elemento com o título \"Para a página seguinte\"\n",
    "# The \"click\" in selenium was made from localization of element \"Para a página seguinte\"\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"a[title='Para a página seguinte']\")))\n",
    "            page_html = driver.page_source     \n",
    "            pageSoup = BeautifulSoup(page_html, \"html.parser\")\n",
    "            \n",
    "# A partir da análise do HTML feita pelo BeautfulSoup, ele irá raspar os dados que informamos.\n",
    "# With the HTML analysis complete for BeautfulSoup, we can start extract the informations.\n",
    "# 1) O nome dos jogadores, tendo como elemento âncora a classe hauptlink dentro da tabela principal da página\n",
    "# 1) The players, using like ancor element the hauptlink class inside a major table in the page.\n",
    "            Jogadores = []\n",
    "            nomes = pageSoup.find_all(lambda tag: tag.name == \"td\" and tag.get(\"class\") == [\"hauptlink\"])\n",
    "            for i in range (0, 25):\n",
    "                Jogadores.append(nomes[i].text)\n",
    "            Nome_do_Jogador = [jogador.strip() for jogador in Jogadores]\n",
    "\n",
    "# 2) A quantidade de gols marcadas, tendo como elemento âncora a classe zentriert hauptlink dentro da tabela principal da página\n",
    "# 2) The second information is the goals scored. For this, I used the zentriert hauptlink like ancor element.\n",
    "\n",
    "            Quantidade_de_Gols=[]\n",
    "            Gols = pageSoup.find_all(\"td\", {\"class\": \"zentriert hauptlink\"})\n",
    "            for i in range (0, 25):\n",
    "                Quantidade_de_Gols.append(Gols[i].text)\n",
    "                \n",
    "# 3) As imagens dos jogadores que serão tratadas posteriormente pelo Deepface. Aqui, importamos o url onde estão essas imagens.\n",
    "# Note que ao invés de definimos uma classe em HTML, inserimos a infomração \"alt: [Nome_do_Jogador]\". Isso porque a classe\n",
    "# dessas URLs era a mesma da classe dos jogadores, o que impedia a extração. Por isso colocamos o título dessas URLs como \n",
    "# parâmetro da extração\n",
    "# Em uma temporada específica, o código travava em duas imagens sem data-src, o que gerava um problema que não consegui resolver,\n",
    "#aparentemente um bug. Por isso inclui uma condicional para, caso não fosse encontrado o data-src, fosse inserido um valor específico\n",
    "# 3) The images that will be used later by Deepface. Here, we import the URLs where these images are located.\n",
    "# Instead of defining an HTML class, we inserted the information \"alt:[player_list]\". This is because the class of these URLS\n",
    "# was the same as the \"players names\", wich prevented the extraction. Therefore, we used the titles of these URLs as extraction\n",
    "# parameters.\n",
    "# In a specific season, the code would get stuck on two images without a data-src atribute, causing an issue that we couldn't\n",
    "# resolve, possibly a bug. As a solution, I included a conditional statement to check if the \"data-src\" was not found, and in \n",
    "# that case, a specific value would be inserted\n",
    "            valor_padrao = \"Link não encontrado\"\n",
    "            imagens_jogadores = pageSoup.find_all(\"img\", {\"alt\": [Nome_do_Jogador]})\n",
    "            urls_imagens = [img.get(\"data-src\") or valor_padrao for img in imagens_jogadores]\n",
    "            for url_imagem in urls_imagens:\n",
    "                Foto = urls_imagens\n",
    "\n",
    "# Após as extraçãoes, as informações são adicionadas às listas criadas antes do código de raspagem\n",
    "# After the extractions, the information will be added to the lists created before the scraping code\n",
    "            Lista_de_Jogadores.extend(Nome_do_Jogador)\n",
    "            Fotos.extend(urls_imagens)\n",
    "            Gols_Marcados.extend(Quantidade_de_Gols)\n",
    "\n",
    "# Ao final do processo, o Selenium espera 5 segundos e busca o \"elemento\" necessário para realizar um click e passar para a \n",
    "# próxima página\n",
    "# At the end of the process, Selenium waits 5 seconds and searchs for the necessary \"element\" to perform a click and move to \n",
    "# the next page\n",
    "            next_page_element.click()\n",
    "            time.sleep(5)\n",
    "            page_html = driver.page_source\n",
    "\n",
    "# Aqui ele faz uma nova leitura da nova página e retorna o loop, caso verdadeiro\n",
    "# Here, the code performs a new read of the page and returns to the loop if it is true\n",
    "            pageSoup = BeautifulSoup(page_html, \"html.parser\")\n",
    "\n",
    "# Caso seja falso, ele entra na exceção causada pelo TimeoutException\n",
    "# If it is false, it enters the exception caused by TimeoutException\n",
    "        except TimeoutException as e:\n",
    "\n",
    "# Para impedir que uma mensagem de erro seja mostrada na tela, inserimos uma mensagem personalizada seguida do break\n",
    "# To prevent an error message from being displayer, I inserted a customized message followed by the break command\n",
    "            e = \"Busca finalizada\"\n",
    "            print(e)\n",
    "            break\n",
    "\n",
    "# Após o break, o sistema faz uma extração na última página. Nessa parte foi feita uma pequena alteração no código. Como não \n",
    "# sabemos quantos jogadores aparecem na última página, limitamos a quantidade de extração dos \"Jogadores\" pelo tamanho da lista\n",
    "# de Quantidades de Gols.\n",
    "# After the break, the system performs an extraction on the last page. In this part, a small modification was made to the code.\n",
    "# Since we don't know how many players appear on the last page, we limited the extraction of \"Players\" based on the size of \n",
    "# the \"Goals Quantity\" list\n",
    "    page_html = driver.page_source\n",
    "    pageSoup = BeautifulSoup(page_html, \"html.parser\")\n",
    "\n",
    "    Quantidade_de_Gols = []\n",
    "    Gols = pageSoup.find_all(\"td\", {\"class\": \"zentriert hauptlink\"})\n",
    "    for Gol in Gols:\n",
    "        Quantidade_de_Gols.append(Gol.text)\n",
    "\n",
    "    Jogadores = []\n",
    "    nomes = pageSoup.find_all(lambda tag: tag.name == \"td\" and tag.get(\"class\") == [\"hauptlink\"])\n",
    "    for i in range(0, len(Quantidade_de_Gols)):\n",
    "        Jogadores.append(nomes[i].text)\n",
    "    Nome_do_Jogador = [jogador.strip() for jogador in Jogadores]\n",
    "    \n",
    "    valor_padrao = \"Link não encontrado\"\n",
    "    imagens_jogadores = pageSoup.find_all(\"img\", {\"alt\": [Nome_do_Jogador]})\n",
    "    urls_imagens = [img.get(\"data-src\") or valor_padrao for img in imagens_jogadores]\n",
    "\n",
    "    Lista_de_Jogadores.extend(Nome_do_Jogador)\n",
    "    Gols_Marcados.extend(Quantidade_de_Gols)\n",
    "    Fotos.extend(urls_imagens)\n",
    "\n",
    "# Exibe uma mensagem de conclusão\n",
    "# Show a finish message\n",
    "    print(\"Extração adicional concluída\")\n",
    "    driver.quit()\n",
    "\n",
    "# E oferece ao usuário a opçao de fazer uma nova extração de dados, realizando novamente o loop anterior caso a resposta seja\n",
    "# positiva\n",
    "# Offer a user the option for perform a new extraction.\n",
    "    mais_dados = input(\"Deseja extrair os dados de outra página? Responda sim ou não.\\n\")\n",
    "    resposta = mais_dados.lower()\n",
    "    if resposta == \"sim\":\n",
    "        print(\"OK\")\n",
    "    else:\n",
    "        print(\"Ok, agora vamos montar a tabela com os resultados extraídos\")\n",
    "extraçao ()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6976da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nessa parte retirei as importações defeituosas dos links das imagens\n",
    "# In this part, we removed the faulty imports of the the urls_images\n",
    "\n",
    "Fotos = [foto for foto in Fotos if foto != \"Link não encontrado\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce0adc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Após a extração dos dados, as listas são incluídas em uma tabela inicial e mesclada a partir da coluna Jogadores\n",
    "# Aqui, somamos os goals e unimos as URLs das fotos\n",
    "# After the data extraction, the lists are inserted into an initial table and merged based on the \"Jogadores\" colunm using an \n",
    "# anchor element. Here, we sum the goals and combine the photo URLs\n",
    "Gols_marcados = [int(valor) for valor in Gols_Marcados]\n",
    "\n",
    "df = pd.DataFrame({\"Jogadores\": Lista_de_Jogadores, \"Gols\": Gols_marcados, \"Links das Fotos\": Fotos})\n",
    "df_agrupado = df.groupby('Jogadores')['Gols'].sum().reset_index()\n",
    "df_unido = df.groupby('Jogadores')['Links das Fotos'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "resultado = pd.merge(df_agrupado, df_unido, on='Jogadores').sort_values(by=\"Gols\", ascending = False)\n",
    "\n",
    "df_tabela = pd.DataFrame({\n",
    "    'Jogadores': resultado['Jogadores'],\n",
    "    'Gols': resultado['Gols'],\n",
    "    'Links das Fotos': resultado['Links das Fotos']\n",
    "})\n",
    "\n",
    "# Para não haver mais de uma URL para cada jogador, excluimos todos os elementos após a ',' de cada linha dos Links das Fotos\n",
    "# To avoid having multiple URLs for each player, we removed all elements after the comma (',') in each row of the photo links.\n",
    "df_tabela['Links das Fotos'] = df_tabela['Links das Fotos'].apply(lambda x: x.split(',')[0])\n",
    "\n",
    "\n",
    "# Exibir o DataFrame como tabela\n",
    "# Show a example Dataframe\n",
    "print(df_tabela.to_string)\n",
    "\n",
    "# Exporting the DataFrame as a temporary table in Excel\n",
    "\n",
    "df_tabela.to_excel('Jogadores e etnias entre os artilheiros da La Liga.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32049cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apos estarmos com a tabela criada, é hora de iniciarmos a utilização do Deepface para classificarmos, a partir da IA, a etnia \n",
    "# dos jogadores\n",
    "# Once we have the table created, it's time to start using Deepface to classify the players' ethnicity based on AI.\n",
    "\n",
    "\n",
    "lista_de_fotos = []\n",
    "dataframe = pd.read_excel('Jogadores e etnias entre os artilheiros da La Liga.xlsx')\n",
    "coluna = dataframe['Links das Fotos']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação do módulo necessário do DeepFace para a análise \n",
    "# Importing the necessary module from DeepFace for analysis.\n",
    "\n",
    "from deepface import DeepFace\n",
    "\n",
    "# Com a lista anteriormente criada, o DeepFace e o Numpy vão abrir os links individualmente, entregar para a IA, fazer a análise\n",
    "# e retornar uma nova lista com os resultados\n",
    "# With the previously created list, DeepFace and Numpy will open the links individually, provide them to the AI for analysis, \n",
    "# and return a new list with the results.\n",
    "\n",
    "lista_etnias = []\n",
    "\n",
    "# Inseri essa contagem para que possamos acompanhar o progresso da análise, já que demora um pouco\n",
    "# I added this counter so that we can track the progress of the analysis, as it takes some time.\n",
    "contagem = 0\n",
    "\n",
    "for image_url in coluna:\n",
    "    try:\n",
    "        etnias = []\n",
    "        response = requests.get (image_url)\n",
    "        image_array = np.asarray(bytearray(response.content), dtype=np.uint8)\n",
    "        image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
    "        temp_filename = \"imagem.jpg\"\n",
    "        cv2.imwrite(temp_filename, image)\n",
    "        analise = DeepFace.analyze(img_path=temp_filename, actions = ('race',))\n",
    "        etnias = analise[0]['dominant_race']\n",
    "        lista_etnias.append(etnias)\n",
    "        contagem +=1\n",
    "        print(contagem)\n",
    "    except:\n",
    "\n",
    "# Alguns jogadores não tem foto ou a IA não consegue, a partir de seus cálculos, definir um elemento preponderante entre as \n",
    "# etnias. Quando isso ocorrer, o programa deve inserir a classificação informada ao invés do erro.\n",
    "# Some players do not have a photo or the AI cannot determine a predominant ethnicity based on its calculations. When this \n",
    "# happens, the program should insert the provided classification instead of the error.\n",
    "\n",
    "        etnias = \"Não detectada\"\n",
    "        lista_etnias.append(etnias)\n",
    "        contagem +=1\n",
    "        print(contagem)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ao finalizar a análise, o macro pega os dados gerado pela análise e adiciona na tabela em sua posição correspondente\n",
    "# Upon completion of the analysis, the macro takes the data generated by the analysis and adds it to the table in their \n",
    "# corresponding positions.\n",
    "\n",
    "dataframe = pd.read_excel('Jogadores e etnias entre os artilheiros da La Liga.xlsx')\n",
    "dataframe['Etnias'] = lista_etnias\n",
    "dataframe.to_excel('Jogadores e etnias entre os artilheiros da La Liga.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
